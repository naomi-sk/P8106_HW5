---
title: "P8106_HW5"
author:
- "Naomi Simon-Kumar"
- ns3782
date: "23/11/2025"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading libraries

```{r libraries, message=FALSE, warning=FALSE}

# Load libraries
library(tidyverse)
library(caret)
library(ggplot2)  
library(tidymodels)
library(e1071)

```

# Question 1. Support Vector Machines

## Partition into training and testing set

```{r}

# Read in dataset
auto <- read.csv("auto.csv")

# Remove NAs
auto <- na.omit(auto)

# Make sure factor variables are correctly coded
auto$cylinders <- factor(auto$cylinders)
auto$origin <- factor(auto$origin)
auto$mpg_cat <- factor(auto$mpg_cat, levels = c("low", "high"))

# Check variable types
str(auto)
levels(auto$mpg_cat)

# Set seed for reproducibility
set.seed(299)

# Split data into training and testing data
data_split_auto <- initial_split(auto, prop = 0.7)


# Extract the training and test data
training_data_auto <- training(data_split_auto)
testing_data_auto <- testing(data_split_auto)

# Check variable types
# str(training_data_auto)
# str(testing_data_auto)

```

I made sure to recode the variables origin and cylinders to factor variable type. Although cylinders was originally represented as an integer, it is a multi-valued discrete variable as its values represent categorical groupings of engine types (i.e., 4, 6, 8 cylinder),

## a) Fit support vector classifier

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit model
linear.tune <- tune.svm(mpg_cat ~ . ,
                        data = training_data_auto,
                        kernel = "linear",
                        cost = exp(seq(-6,3, len = 50)),
                        scale = TRUE)

# Tuning curve
plot(linear.tune) 


```

I initially proceeded with exploring a wide grid for the cost tuning parameter, from exp(-6) to exp(3). 
However, the plot shows that accuracy (i.e., 1-Misclassification Error) stabilises quite early, around cost = 1, and increasing cost beyond that does not notably improve performance. Therefore, I decided on reducing the size of the cost tuning parameter grid.

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit model
linear.tune.2 <- tune.svm(mpg_cat ~ . ,
                        data = training_data_auto,
                        kernel = "linear",
                        cost = exp(seq(-6,2, len = 50)),
                        scale = TRUE)

# Tuning curve
plot(linear.tune.2) 

# Optimal parameters
linear.tune.2$best.parameters

exp(-6)
exp(2)
```

I refined the tuning parameter grid to cover values in a range between exp(-6) and exp(-2), which appears to be appropriate.


## a) Fit support vector machine with radial kernel

```{r}


```
