---
title: "P8106_HW5"
author:
- "Naomi Simon-Kumar"
- ns3782
date: "23/11/2025"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading libraries

```{r libraries, message=FALSE, warning=FALSE}

# Load libraries
library(tidyverse)
library(caret)
library(ggplot2)  
library(tidymodels)
library(e1071)

```

# Question 1. Support Vector Machines

## Partition into training and testing set

```{r}

# Read in dataset
auto <- read.csv("auto.csv")

# Remove NAs
auto <- na.omit(auto)

# Make sure factor variables are correctly coded
auto$cylinders <- factor(auto$cylinders)
auto$origin <- factor(auto$origin)
auto$mpg_cat <- factor(auto$mpg_cat, levels = c("low", "high"))

# Check variable types
str(auto)
levels(auto$mpg_cat)

# Set seed for reproducibility
set.seed(299)

# Split data into training and testing data
data_split_auto <- initial_split(auto, prop = 0.7)


# Extract the training and test data
training_data_auto <- training(data_split_auto)
testing_data_auto <- testing(data_split_auto)

# Check variable types
# str(training_data_auto)
# str(testing_data_auto)

```

I made sure to recode the variables origin and cylinders to factor variable type. Although cylinders was originally represented as an integer, it is a multi-valued discrete variable as its values represent categorical groupings of engine types (i.e., 4, 6, 8 cylinder),

## a) Fit support vector classifier

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit model
linear.tune <- tune.svm(mpg_cat ~ . ,
                        data = training_data_auto,
                        kernel = "linear",
                        cost = exp(seq(-6,3, len = 50)),
                        scale = TRUE)

# Tuning curve
plot(linear.tune) 

```

I initially proceeded with exploring a wide grid for the cost tuning parameter, from exp(-6) to exp(3). 
However, the plot shows that accuracy (i.e., 1-Misclassification Error) stabilises quite early, around cost = 1, and increasing cost beyond that does not notably improve performance. Therefore, I decided on reducing the size of the cost tuning parameter grid.

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit model
linear.tune.2 <- tune.svm(mpg_cat ~ . ,
                        data = training_data_auto,
                        kernel = "linear",
                        cost = exp(seq(-6,2, len = 50)),
                        scale = TRUE)

# Tuning curve
plot(linear.tune.2) 

# Optimal parameters
linear.tune.2$best.parameters

```

I refined the tuning parameter grid to cover values in a range between exp(-6) and exp(2), which appears to be appropriate. The **best cost tuning parameter is 0.239651** which is within this range and not at the edge of the grid boundaries. This is also confirmed by the plotted tuning curve, where the minimum cross validation accuracy (1-Misclassification error) is around 0.08.

Next, finding the training error:

```{r}

# Set seed for reproducibility
set.seed(299)

# get the best model
best.linear <- linear.tune.2$best.model

# Training Error
pred_train <- predict(best.linear, newdata = training_data_auto)
confusionMatrix(data = pred_train,
                reference = training_data_auto$mpg_cat)

# Calculating training misclassification error

1-0.9161 # 0.0839

# Test Error
pred_test <- predict(best.linear, newdata = testing_data_auto)
confusionMatrix(data = pred_test,
                reference = testing_data_auto$mpg_cat)


# Calculating test misclassification error

1-0.8898 # 0.1102


```

Based on the model, the training misclassification error is **0.0839 (8.39%)** with **accuracy = 0.9161**, which shows that there is strong model performance. **The Kappa statistic is 0.8319**, which indicates an excellent level of agreement between predicted, and observed mpg categories beyond chance (McHugh, 2012).

The test misclassification error is **0.1102 (11.02%)** with **accuracy = 0.8898**, which indicates that our model generalises well to unseen data. **The Kappa statistic is 0.7794**, indicating substantial agreement between predicted and observed mpg categories, beyond what would be expected due to chance (McHugh, 2012). 


## b) Fit support vector machine with radial kernel

```{r}


```


# References

McHugh, M. L. (2012). Interrater reliability: the kappa statistic. Biochemia medica, 22(3), 276-282.