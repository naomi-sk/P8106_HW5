---
title: "P8106_HW5"
author:
- "Naomi Simon-Kumar"
- ns3782
date: "23/11/2025"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading libraries

```{r libraries, message=FALSE, warning=FALSE}

# Load libraries
library(tidyverse)
library(caret)
library(ggplot2)  
library(tidymodels)
library(e1071)
library(factoextra)
library(gridExtra)
library(corrplot)
library(RColorBrewer)
library(gplots)
library(jpeg)

```

# Question 1. Support Vector Machines

## Partition into training and testing set

```{r}

# Read in dataset
auto <- read.csv("auto.csv")

# Remove NAs
auto <- na.omit(auto)

# Make sure factor variables are correctly coded
auto$cylinders <- factor(auto$cylinders)
auto$origin <- factor(auto$origin)
auto$mpg_cat <- factor(auto$mpg_cat, levels = c("low", "high"))

# Check variable types
str(auto)
levels(auto$mpg_cat)

# Set seed for reproducibility
set.seed(299)

# Split data into training and testing data
data_split_auto <- initial_split(auto, prop = 0.7)


# Extract the training and test data
training_data_auto <- training(data_split_auto)
testing_data_auto <- testing(data_split_auto)

# Check variable types
# str(training_data_auto)
# str(testing_data_auto)

```

I made sure to recode the variables origin and cylinders to factor variable type. Although cylinders was originally represented as an integer, it is a multi-valued discrete variable as its values represent categorical groupings of engine types (i.e., 4, 6, 8 cylinder),

## a) Fit support vector classifier

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit model
linear.tune <- tune.svm(mpg_cat ~ . ,
                        data = training_data_auto,
                        kernel = "linear",
                        cost = exp(seq(-6,3, len = 50)),
                        scale = TRUE)

# Tuning curve
plot(linear.tune) 

```

I initially proceeded with exploring a wide grid for the cost tuning parameter, from exp(-6) to exp(3). 
However, the plot shows that accuracy (i.e., 1-Misclassification Error) stabilises quite early, around cost = 1, and increasing cost beyond that does not notably improve performance. Therefore, I decided on reducing the size of the cost tuning parameter grid.

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit model
linear.tune.2 <- tune.svm(mpg_cat ~ . ,
                        data = training_data_auto,
                        kernel = "linear",
                        cost = exp(seq(-6,2, len = 50)),
                        scale = TRUE)

# Tuning curve
plot(linear.tune.2) 

# Optimal parameters
linear.tune.2$best.parameters

```

I refined the tuning parameter grid to cover values in a range between exp(-6) and exp(2), which appears to be appropriate. The **best cost tuning parameter is 0.239651** which is within this range and not at the edge of the grid boundaries. This is also confirmed by the plotted tuning curve, where the minimum cross validation accuracy (1-Misclassification error) is around 0.08.

Next, finding the training error:

```{r}

# Set seed for reproducibility
set.seed(299)

# get the best model
best.linear <- linear.tune.2$best.model

# Training Error
pred_train <- predict(best.linear, newdata = training_data_auto)
confusionMatrix(data = pred_train,
                reference = training_data_auto$mpg_cat)

# Calculating training misclassification error

1-0.9161 # 0.0839

# Test Error
pred_test <- predict(best.linear, newdata = testing_data_auto)
confusionMatrix(data = pred_test,
                reference = testing_data_auto$mpg_cat)


# Calculating test misclassification error

1-0.8898 # 0.1102


```

Based on the model, the training misclassification error is **0.0839** with **accuracy = 0.9161**, which shows that there is strong model performance (i.e., the model is a good fit to the training data). **The Kappa statistic is 0.8319**, which indicates an excellent level of agreement between predicted, and observed mpg categories beyond chance (McHugh, 2012).

The test misclassification error is **0.1102** with **accuracy = 0.8898**, which indicates that our model generalises well to unseen data. **The Kappa statistic is 0.7794**, indicating substantial agreement between predicted and observed mpg categories, beyond what would be expected due to chance (McHugh, 2012). 

## b) Fit support vector machine with radial kernel

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit initial radial kernel model
# 2 tuning parameters: cost and gamma
# radial.tune <- tune.svm(mpg_cat ~ . ,
 #                       data = training_data_auto,
 #                       kernel = "radial",
 #                       cost = exp(seq(1, 5, len = 50)),
 #                       gamma = exp(seq(-10, 0,len = 20)))


# Best cost and gamma parameters
# radial.tune$best.parameters
# gamma = 0.3490181  , cost = 5.667193

# 2.718 to 148.4132 : cost
#  4.539993e-05 to 1 : gamma

# Fit refined radial kernel model
radial.tune <- tune.svm(mpg_cat ~ . ,
                      data = training_data_auto,
                      kernel = "radial",
                      cost = exp(seq(1, 4, len = 50)),
                      gamma = exp(seq(-8, 3,len = 20)))

# Checking to see if tuning parameters are within grid
exp(1) # 2.718282
exp(4) # 54.59815

exp(-8) # 0.0003354626
exp(3)# 20.08554

# Find best tuning parameters
radial.tune$best.parameters

# Plot 
plot(radial.tune, transform.y = log, transform.x = log,
     color.palette = terrain.colors)

```

I fit an initial support vector machine with radial kernel model, exploring different grids for the tuning parameters cost and gamma to ensure the optimal tuning parameters were appropriately within their respective grid ranges and not at the boundary of the grid. The best **cost** tuning parameter for this model is **5.33063**, and the best **gamma** tuning parameter is **0.3490181**. 

Next, finding the testing and training error:

```{r}

# Set seed for reproducibility
set.seed(299)

# get the best model
best.radial <- radial.tune$best.model

# Training Error and Confusion Matrix
pred.radial.train <- predict(best.radial, 
                       newdata = training_data_auto)

confusionMatrix(data = pred.radial.train,
                reference = training_data_auto$mpg_cat)

1-0.9672 # misclassification training error          

# Testing Error and Confusion Matrix
pred.radial.test <- predict(best.radial, 
                       newdata = testing_data_auto)

confusionMatrix(data = pred.radial.test,
                reference = testing_data_auto$mpg_cat)

1-0.9237 # misclassification testing error     

```

Based on the model, the training misclassification error is **0.0328** with **accuracy = 0.9672**, indicating excellent model performance on the training data. **The Kappa statistic is 0.9342**, which reflects an excellent level of agreement between predicted and observed mpg categories beyond chance (McHugh, 2012).

The test misclassification error is **0.0763** with **accuracy = 0.9237**, which indicates that our model generalises well to unseen data. **The Kappa statistic is 0.8469**, indicating substantial agreement between predicted and observed mpg categories, beyond what would be expected due to chance (McHugh, 2012).


```{r}

# Set seed for reproducibility
set.seed(299)

# Plot radial kernel Support vector machine decision boundary
plot(best.radial, training_data_auto,
     weight ~ horsepower,
     slice = list(acceleration = 12.2, displacement = 97, year = 76, origin = 2, cylinders = 4),
     grid = 100,
     symbolPalette = c("cyan", "darkblue"),
     color.palette = heat.colors)

```

I plotted the radial support vector machine decision boundary, using the predictor variables horsepower and weight from the training data set, keeping the other predictors fixed. This allows us to see that the decision boundary is nonlinear.


# Question 2.

## Load data

```{r}

# Load library
library(ISLR)

# Load data
data("USArrests")
USArrests_data <- USArrests

```


## a) Hierarchical clustering Part 1

I proceeded with performing hierarchical clustering using complete linkage and Euclidean distance. With complete linkage, we typically get more compact clusters.

```{r}

# Set seed for reproducibility
set.seed(299)

# Hierarchical clustering using complete linkage and Euclidean distance
hc.complete <- hclust(dist(USArrests_data), method = "complete")


fviz_dend(hc.complete, k = 4,
          cex = 0.3,
          palette = "jco", # color scheme
          color_labels_by_k = TRUE,
          rect = TRUE, # add a rectangle around groups
          rect_fill = TRUE,
          rect_border = "jco",
          labels_track_height = 2.5)

```

Based on the visualised dendrogram, we can see that there are three more evenly sized clusters, as well as a single small cluster with only two states (Florida and North Carolina). When inspecting the data, it appears as though these states have very high assault rates compared to most other states, in addition to high murder rates, which may be an explanation for their grouping.
The other larger clusters may have similar high arrests across multiple crime categories and similar urban population density.  For example, the red cluster, includes Iowa, Nebraska, North Dakota and may reflect lower crime rates and moderate to lower urban population, compared to yellow cluster inclusive of higher urban populations such as California and New York, which also have higher crime rates in several categories.

```{r}

# Set seed for reproducibility
set.seed(299)

# Cut into 3 clusters
ind3.complete <- cutree(hc.complete, 3)


# Inspect Cluster 1
dat[ind4.complete == 1,]

# Inspect Cluster 2
dat[ind4.complete == 2,]

# Inspect Cluster 3
dat[ind4.complete == 3,]

```

From cutting the dendrogram, we see **Cluster 1** includes the following states: Alabama, Alaska, Arizona, California, Delaware, Florida, Illinois, Louisiana, Maryland, Michigan, Mississippi, Nevada, New Mexico, New York, North Carolina, South Carolina. These may tend to have higher rates of crimes such as Assault and Murder, as well as having higher urban populations.

**Cluster 2** includes the following states: Arkansas, Colorado, Georgia, Massachusetts, Missouri, New Jersey, Oklahoma, Oregon, Rhode Island, Texas, Virginia, Washington, Wyoming, Tennessee. These states may have more moderate/less extreme crime levels across the categories, and potentially more of a mixed profile across the variables, including urban population.

Finally, **Cluster 3** includes the states: Connecticut, Hawaii, Idaho, Indiana, Iowa, Kansas, Kentucky, Maine, Minnesota, Montana, Nebraska, New Hampshire, North Dakota, Ohio, Pennsylvania, South Dakota, Utah, Vermont, West Virginia, Wisconsin. These may represent states with comparably lower crime rates as well as lower urban populations.


```{r}

col1 <- colorRampPalette(brewer.pal(9, "GnBu"))(100)
col2 <- c("yellow","blue4")


# Make matrix based on data
data_matrix <- as.matrix(USArrests_data[, 1:4])
rownames(data_matrix) <- rownames(USArrests_data)

# Heatmap 
heatmap.2(t(data_matrix),
          col = col1, keysize=.8, key.par = list(cex=.5),
          trace = "none", key = TRUE, 
          cexCol = 0.6,        # Shrink state names
          cexRow = 0.8,        # Optional: variable font
          labCol = rownames(USArrests_data),
          margins = c(12, 8))


```

From the heatmap, which does not represent all the states in the dataset, we see confirmation that states including North Carolina, Delaware, Louisiana, New York and Mississippi (dark blue colouring). Most states have relatively low and comparable Murder and Rape crime rates. Urban population also doesn't appear to be highly variable. Therefore, it seems as though the clustering is largely influenced by the Assault rates.

## a) Hierarchical clustering Part 2

```{r}



```



# References

McHugh, M. L. (2012). Interrater reliability: the kappa statistic. Biochemia medica, 22(3), 276-282.