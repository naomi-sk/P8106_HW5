---
title: "P8106_HW5"
author:
- "Naomi Simon-Kumar"
- ns3782
date: "23/11/2025"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading libraries

```{r libraries, message=FALSE, warning=FALSE}

# Load libraries
library(tidyverse)
library(caret)
library(ggplot2)  
library(tidymodels)
library(e1071)

```

# Question 1. Support Vector Machines

## Partition into training and testing set

```{r}

# Read in dataset
auto <- read.csv("auto.csv")

# Remove NAs
auto <- na.omit(auto)

# Make sure factor variables are correctly coded
auto$cylinders <- factor(auto$cylinders)
auto$origin <- factor(auto$origin)
auto$mpg_cat <- factor(auto$mpg_cat, levels = c("low", "high"))

# Check variable types
str(auto)
levels(auto$mpg_cat)

# Set seed for reproducibility
set.seed(299)

# Split data into training and testing data
data_split_auto <- initial_split(auto, prop = 0.7)


# Extract the training and test data
training_data_auto <- training(data_split_auto)
testing_data_auto <- testing(data_split_auto)

# Check variable types
# str(training_data_auto)
# str(testing_data_auto)

```

I made sure to recode the variables origin and cylinders to factor variable type. Although cylinders was originally represented as an integer, it is a multi-valued discrete variable as its values represent categorical groupings of engine types (i.e., 4, 6, 8 cylinder),

## a) Fit support vector classifier

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit model
linear.tune <- tune.svm(mpg_cat ~ . ,
                        data = training_data_auto,
                        kernel = "linear",
                        cost = exp(seq(-6,3, len = 50)),
                        scale = TRUE)

# Tuning curve
plot(linear.tune) 

```

I initially proceeded with exploring a wide grid for the cost tuning parameter, from exp(-6) to exp(3). 
However, the plot shows that accuracy (i.e., 1-Misclassification Error) stabilises quite early, around cost = 1, and increasing cost beyond that does not notably improve performance. Therefore, I decided on reducing the size of the cost tuning parameter grid.

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit model
linear.tune.2 <- tune.svm(mpg_cat ~ . ,
                        data = training_data_auto,
                        kernel = "linear",
                        cost = exp(seq(-6,2, len = 50)),
                        scale = TRUE)

# Tuning curve
plot(linear.tune.2) 

# Optimal parameters
linear.tune.2$best.parameters

```

I refined the tuning parameter grid to cover values in a range between exp(-6) and exp(2), which appears to be appropriate. The **best cost tuning parameter is 0.239651** which is within this range and not at the edge of the grid boundaries. This is also confirmed by the plotted tuning curve, where the minimum cross validation accuracy (1-Misclassification error) is around 0.08.

Next, finding the training error:

```{r}

# Set seed for reproducibility
set.seed(299)

# get the best model
best.linear <- linear.tune.2$best.model

# Training Error
pred_train <- predict(best.linear, newdata = training_data_auto)
confusionMatrix(data = pred_train,
                reference = training_data_auto$mpg_cat)

# Calculating training misclassification error

1-0.9161 # 0.0839

# Test Error
pred_test <- predict(best.linear, newdata = testing_data_auto)
confusionMatrix(data = pred_test,
                reference = testing_data_auto$mpg_cat)


# Calculating test misclassification error

1-0.8898 # 0.1102


```

Based on the model, the training misclassification error is **0.0839** with **accuracy = 0.9161**, which shows that there is strong model performance (i.e., the model is a good fit to the training data). **The Kappa statistic is 0.8319**, which indicates an excellent level of agreement between predicted, and observed mpg categories beyond chance (McHugh, 2012).

The test misclassification error is **0.1102** with **accuracy = 0.8898**, which indicates that our model generalises well to unseen data. **The Kappa statistic is 0.7794**, indicating substantial agreement between predicted and observed mpg categories, beyond what would be expected due to chance (McHugh, 2012). 

## b) Fit support vector machine with radial kernel

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit initial radial kernel model
# 2 tuning parameters: cost and gamma
# radial.tune <- tune.svm(mpg_cat ~ . ,
 #                       data = training_data_auto,
 #                       kernel = "radial",
 #                       cost = exp(seq(1, 5, len = 50)),
 #                       gamma = exp(seq(-10, 0,len = 20)))


# Best cost and gamma parameters
# radial.tune$best.parameters
# gamma = 0.3490181  , cost = 5.667193

# 2.718 to 148.4132 : cost
#  4.539993e-05 to 1 : gamma

# Fit refined radial kernel model
radial.tune <- tune.svm(mpg_cat ~ . ,
                      data = training_data_auto,
                      kernel = "radial",
                      cost = exp(seq(1, 4, len = 50)),
                      gamma = exp(seq(-8, 3,len = 20)))

# Checking to see if tuning parameters are within grid
exp(1) # 2.718282
exp(4) # 54.59815

exp(-8) # 0.0003354626
exp(3)# 20.08554

# Find best tuning parameters
radial.tune$best.parameters

# Plot 
plot(radial.tune, transform.y = log, transform.x = log,
     color.palette = terrain.colors)

```

I fit an initial support vector machine with radial kernel model, exploring different grids for the tuning parameters cost and gamma to ensure the optimal tuning parameters were appropriately within their respective grid ranges and not at the boundary of the grid. The best **cost** tuning parameter for this model is **5.33063**, and the best **gamma** tuning parameter is **0.3490181**. 

Next, finding the testing and training error:

```{r}

# Set seed for reproducibility
set.seed(299)

# get the best model
best.radial <- radial.tune$best.model

# Training Error and Confusion Matrix
pred.radial.train <- predict(best.radial, 
                       newdata = training_data_auto)

confusionMatrix(data = pred.radial.train,
                reference = training_data_auto$mpg_cat)

1-0.9672 # misclassification training error          

# Testing Error and Confusion Matrix
pred.radial.test <- predict(best.radial, 
                       newdata = testing_data_auto)

confusionMatrix(data = pred.radial.test,
                reference = testing_data_auto$mpg_cat)

1-0.9237 # misclassification testing error     

```

Based on the model, the training misclassification error is **0.0328** with **accuracy = 0.9672**, indicating excellent model performance on the training data. **The Kappa statistic is 0.9342**, which reflects an excellent level of agreement between predicted and observed mpg categories beyond chance (McHugh, 2012).

The test misclassification error is **0.0763** with **accuracy = 0.9237**, which indicates that our model generalises well to unseen data. **The Kappa statistic is 0.8469**, indicating substantial agreement between predicted and observed mpg categories, beyond what would be expected due to chance (McHugh, 2012).


```{r}

# Set seed for reproducibility
set.seed(299)

# Plot radial kernel Support vector machine decision boundary
plot(best.radial, training_data_auto,
     weight ~ horsepower,
     slice = list(acceleration = 12.2, displacement = 97, year = 76, origin = 2, cylinders = 4),
     grid = 100,
     symbolPalette = c("cyan", "darkblue"),
     color.palette = heat.colors)

```

I plotted the radial support vector machine decision boundary, using the predictor variables horsepower and weight from the training data set, keeping the other predictors fixed. This allows us to see that the decision boundary is nonlinear.

# References

McHugh, M. L. (2012). Interrater reliability: the kappa statistic. Biochemia medica, 22(3), 276-282.